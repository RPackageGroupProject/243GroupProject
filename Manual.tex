\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\setlength{\parindent}{0em}
\setlength{\parskip}{0.5em}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\title{STAT 243 Final Project: Genetic Algorithm for Model Selection}
\author{.....Chris Gagne}
\date{Dec 15 2017}

\maketitle



\section*{Description of Algorithm and Approach to Modularization}

Our package has one main function select(). This does the entire genetic algorithm on a dataset containing rows as examples and columns as variables.

Our approach to modularization was break the genetic algorithm into its natural processes.
(1) initialize population of chromosomes initialize(),
(2) calculation of fitness fitness(),
(3) selection of parents for breeding selection(),
(4) breeding parents and choosing the next generation nextGeneration(). Within this function, we use crossover() to do the actual breeding and mutation().

We allow changes to the general algorithm in a number of ways:

We implemented 3 ways to select the parent generation. The first method was to use fitness-rank-based probabilities to select both parents in the pairs chosen to breed. The second method used fitness-rank-based probabilities to choose just one parent in each pair, and the second parent was chosen randomly. This might help preserved diversity in population, which is important for not converging to local minimum. The third method is tournament selection, which FILL IN HERE. These methods can be toggled between by the user by using the K parameter.

We also implemented a flexible generation gap (the percent of children replacing the parents in each generation) as well as allowed for a user specified number of generations and starting population size.

In terms of represenation, our chromosomes are boolean vectors which are used to index an X-matrix, with columns for each predictor variables. The population set of chromosomes is a matrix of the boolean chromosome vectors and is passed into various functions. nextGeneration() returns a modified version of the population set of chromosomes. We did this, rather than keep old generation chromosomes, to save memory. We do, however, store the fitness scores for each model for each generation that we use to plot the progress of the algorithm (see examples).

\section*{Installation}

Our package can be installed by either downloading and setting the working directory and installing or installing directly from github.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{install}\hlstd{(}\hlstr{'GA'}\hlstd{)}
\hlcom{# or git_install('GA')}
\end{alltt}
\end{kframe}
\end{knitrout}

The package is kept in XYZ's github repo here:
The username for the repo is:

\section*{Tests}

Testing
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(}\hlstr{'testthat'}\hlstd{)}
\hlkwd{test}\hlstd{(}\hlstr{'GA'}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\section*{Example 1:}

Here is a basic example of how to use our package.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(}\hlstr{'GA'}\hlstd{)}
\hlstd{dat}\hlkwb{<-}\hlstd{mtcars} \hlcom{# specifify a data set}
\hlcom{# as the documentation points out,}
\hlcom{# the first column needs to by y and}
\hlcom{# the rest of the columns are possible predictors}

\hlstd{results}\hlkwb{<-}\hlkwd{select}\hlstd{(dat)} \hlcom{# fit}
\end{alltt}
\begin{verbatim}
## [1] "Generation:  10  Fitness:  156.652338825641"
## [1] "Generation:  20  Fitness:  155.476628510258"
## [1] "Generation:  30  Fitness:  155.476628510258"
## [1] "Generation:  40  Fitness:  155.476628510258"
## [1] "Generation:  50  Fitness:  155.476628510258"
## [1] "Generation:  60  Fitness:  155.476628510258"
## [1] "Generation:  70  Fitness:  155.476628510258"
## [1] "Generation:  80  Fitness:  155.476628510258"
## [1] "Generation:  90  Fitness:  155.476628510258"
## [1] "Generation:  100  Fitness:  155.476628510258"
## [1] "ALgorithm Ends"
\end{verbatim}
\end{kframe}
\end{knitrout}

We can inspect the best fitted model.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# inspect fittest model}
\hlkwd{summary}\hlstd{(results}\hlopt{$}\hlstd{fittestModel)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## model(formula = form, data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.9290 -1.5598 -0.5311  1.1850  5.8986 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 38.75179    1.78686  21.687  < 2e-16 ***
## cyl         -0.94162    0.55092  -1.709 0.098480 .  
## hp          -0.01804    0.01188  -1.519 0.140015    
## wt          -3.16697    0.74058  -4.276 0.000199 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.512 on 28 degrees of freedom
## Multiple R-squared:  0.8431,	Adjusted R-squared:  0.8263 
## F-statistic: 50.17 on 3 and 28 DF,  p-value: 2.184e-11
\end{verbatim}
\end{kframe}
\end{knitrout}

As well as see the fitness of the chromosomes as the algorithm proceeds.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plotGA}\hlstd{(results)}
\end{alltt}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in plotGA(results): could not find function "{}plotGA"{}}}\end{kframe}
\end{knitrout}


\section*{Example Comparison against forward/backward selection}

Backwards selection chooses the best model containing regressors for (wt, qseq, am). This seems to be the best model - as it's the lowest we've seen in any iteration of our algorithm. Note that forward selection does not achieve this model.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Comparing against backwards selection}
\hlstd{full}\hlkwb{=}\hlkwd{lm}\hlstd{(dat[,}\hlnum{1}\hlstd{]}\hlopt{~}\hlstd{.,} \hlkwc{data}\hlstd{=dat[,}\hlopt{-}\hlnum{1}\hlstd{])}
\hlkwd{step}\hlstd{(full,} \hlkwc{direction}\hlstd{=}\hlstr{"backward"}\hlstd{,}\hlkwc{trace}\hlstd{=}\hlnum{FALSE}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = dat[, 1] ~ wt + qsec + am, data = dat[, -1])
## 
## Coefficients:
## (Intercept)           wt         qsec           am  
##       9.618       -3.917        1.226        2.936
\end{verbatim}
\end{kframe}
\end{knitrout}


\section*{Example User Inputs}

Here, we show different parameters that we can specifiy for our algorithm. It obtains the fittest model (wt,qsec,am).

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(}\hlstr{'stats'}\hlstd{)}
\hlkwd{library}\hlstd{(}\hlstr{'GA'}\hlstd{)}
\hlcom{#function(fit,...)\{return(extractAIC(fit,...)[2])\}}
\hlstd{results}\hlkwb{<-}\hlkwd{select}\hlstd{(}\hlkwc{dat}\hlstd{=mtcars,}
                \hlkwc{P}\hlstd{=}\hlnum{50}\hlstd{,}
                \hlkwc{model}\hlstd{=glm,}
                \hlkwc{numGens}\hlstd{=}\hlnum{100}\hlstd{,}
                \hlkwc{G}\hlstd{=}\hlnum{0.25}\hlstd{,}
                \hlkwc{method}\hlstd{=}\hlnum{2}\hlstd{,}
                \hlkwc{K}\hlstd{=}\hlnum{2}\hlstd{,}
                \hlkwc{fitnessFunction}\hlstd{=AIC}
                \hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] "Generation:  10  Fitness:  154.119370868901"
## [1] "Generation:  20  Fitness:  154.119370868901"
## [1] "Generation:  30  Fitness:  154.119370868901"
## [1] "Generation:  40  Fitness:  154.119370868901"
## [1] "Generation:  50  Fitness:  154.119370868901"
## [1] "Generation:  60  Fitness:  154.119370868901"
## [1] "Generation:  70  Fitness:  154.119370868901"
## [1] "Generation:  80  Fitness:  154.119370868901"
## [1] "Generation:  90  Fitness:  154.119370868901"
## [1] "Generation:  100  Fitness:  154.119370868901"
## [1] "ALgorithm Ends"
\end{verbatim}
\begin{alltt}
\hlstd{results}\hlopt{$}\hlstd{fittestModel}
\end{alltt}
\begin{verbatim}
## 
## Call:  model(formula = form, data = dat)
## 
## Coefficients:
## (Intercept)           wt         qsec           am  
##       9.618       -3.917        1.226        2.936  
## 
## Degrees of Freedom: 31 Total (i.e. Null);  28 Residual
## Null Deviance:	    1126 
## Residual Deviance: 169.3 	AIC: 154.1
\end{verbatim}
\end{kframe}
\end{knitrout}

We can plot this as well

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plotGA}\hlstd{(results)}
\end{alltt}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in plotGA(results): could not find function "{}plotGA"{}}}\end{kframe}
\end{knitrout}

\section*{Example Early Convergence}

A big difference in our algorithm is the generation Gap. With too high of a generation gap, the algorithm often converges to a local minimum. We would consider adding mutation rate as a flexible parameter in the future.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(}\hlstr{'stats'}\hlstd{)}

\hlstd{results}\hlkwb{<-}\hlkwd{select}\hlstd{(}\hlkwc{dat}\hlstd{=mtcars,}
                \hlkwc{P}\hlstd{=}\hlnum{50}\hlstd{,}
                \hlkwc{model}\hlstd{=glm,}
                \hlkwc{numGens}\hlstd{=}\hlnum{100}\hlstd{,}
                \hlkwc{G}\hlstd{=}\hlnum{0.76}\hlstd{,}
                \hlkwc{method}\hlstd{=}\hlnum{2}\hlstd{,}
                \hlkwc{K}\hlstd{=}\hlnum{2}\hlstd{,}
                \hlkwc{fitnessFunction}\hlstd{=}\hlkwa{function}\hlstd{(}\hlkwc{fit}\hlstd{,}\hlkwc{...}\hlstd{)\{}\hlkwd{return}\hlstd{(}\hlkwd{extractAIC}\hlstd{(fit,...)[}\hlnum{2}\hlstd{])\}}
                \hlstd{)}
\end{alltt}
\begin{verbatim}
## function (formula, family = gaussian, data, weights, subset, 
##     na.action, start = NULL, etastart, mustart, offset, control = list(...), 
##     model = TRUE, method = "glm.fit", x = FALSE, y = TRUE, contrasts = NULL, 
##     ...) 
## {
##     call <- match.call()
##     if (is.character(family)) 
##         family <- get(family, mode = "function", envir = parent.frame())
##     if (is.function(family)) 
##         family <- family()
##     if (is.null(family$family)) {
##         print(family)
##         stop("'family' not recognized")
##     }
##     if (missing(data)) 
##         data <- environment(formula)
##     mf <- match.call(expand.dots = FALSE)
##     m <- match(c("formula", "data", "subset", "weights", "na.action", 
##         "etastart", "mustart", "offset"), names(mf), 0L)
##     mf <- mf[c(1L, m)]
##     mf$drop.unused.levels <- TRUE
##     mf[[1L]] <- quote(stats::model.frame)
##     mf <- eval(mf, parent.frame())
##     if (identical(method, "model.frame")) 
##         return(mf)
##     if (!is.character(method) && !is.function(method)) 
##         stop("invalid 'method' argument")
##     if (identical(method, "glm.fit")) 
##         control <- do.call("glm.control", control)
##     mt <- attr(mf, "terms")
##     Y <- model.response(mf, "any")
##     if (length(dim(Y)) == 1L) {
##         nm <- rownames(Y)
##         dim(Y) <- NULL
##         if (!is.null(nm)) 
##             names(Y) <- nm
##     }
##     X <- if (!is.empty.model(mt)) 
##         model.matrix(mt, mf, contrasts)
##     else matrix(, NROW(Y), 0L)
##     weights <- as.vector(model.weights(mf))
##     if (!is.null(weights) && !is.numeric(weights)) 
##         stop("'weights' must be a numeric vector")
##     if (!is.null(weights) && any(weights < 0)) 
##         stop("negative weights not allowed")
##     offset <- as.vector(model.offset(mf))
##     if (!is.null(offset)) {
##         if (length(offset) != NROW(Y)) 
##             stop(gettextf("number of offsets is %d should equal %d (number of observations)", 
##                 length(offset), NROW(Y)), domain = NA)
##     }
##     mustart <- model.extract(mf, "mustart")
##     etastart <- model.extract(mf, "etastart")
##     fit <- eval(call(if (is.function(method)) "method" else method, 
##         x = X, y = Y, weights = weights, start = start, etastart = etastart, 
##         mustart = mustart, offset = offset, family = family, 
##         control = control, intercept = attr(mt, "intercept") > 
##             0L))
##     if (length(offset) && attr(mt, "intercept") > 0L) {
##         fit2 <- eval(call(if (is.function(method)) "method" else method, 
##             x = X[, "(Intercept)", drop = FALSE], y = Y, weights = weights, 
##             offset = offset, family = family, control = control, 
##             intercept = TRUE))
##         if (!fit2$converged) 
##             warning("fitting to calculate the null deviance did not converge -- increase 'maxit'?")
##         fit$null.deviance <- fit2$deviance
##     }
##     if (model) 
##         fit$model <- mf
##     fit$na.action <- attr(mf, "na.action")
##     if (x) 
##         fit$x <- X
##     if (!y) 
##         fit$y <- NULL
##     fit <- c(fit, list(call = call, formula = formula, terms = mt, 
##         data = data, offset = offset, control = control, method = method, 
##         contrasts = attr(X, "contrasts"), xlevels = .getXlevels(mt, 
##             mf)))
##     class(fit) <- c(fit$class, c("glm", "lm"))
##     fit
## }
## <bytecode: 0x7ffad1042b88>
## <environment: namespace:stats>
\end{verbatim}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in initialization(C = C, P = P): could not find function "{}initialization"{}}}\begin{alltt}
\hlstd{results}\hlopt{$}\hlstd{fittestModel}
\end{alltt}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval(expr, envir, enclos): object 'results' not found}}\end{kframe}
\end{knitrout}

We can plot this as well
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plotGA}\hlstd{(results)}
\end{alltt}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in plotGA(results): could not find function "{}plotGA"{}}}\end{kframe}
\end{knitrout}



\section*{Contributions of Team Members}
- design of algorithm (all members)
- wrapper function (Z,C)
- selection ...
- package organization (chris)
- tests ...
- pdf document...


\end{document}
